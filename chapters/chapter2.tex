%-*-coding: utf-8-*-

\chapter{Описание предложенного решения}

В данной главе будут рассмотрены предложенные идеи архитектур нейронных сетей на основе дерева разбора предложения.


\section{Предложенные идеи}

\section{Формальное описание алгоритма}

\subsection{Предобработка данных}
Для того чтобы алгоритм смог проанализировать предложение, 
его необходимо привести к определенному формату. Мы рассмотрим обработку
тренировочных данных, обработка тестовых данных производится аналогично.

В первую очередь, предложение необходимо разбить на слова. 
Это можно сделать используя регулярные выражения. 

Затем по данному корпусу предложений необходимо построить словарь слов.
Словарь~--- это некоторое подмножество слов предложений корпуса.
Это делается для того, чтобы ограничить объем вычислений, так как использование всех
слов языка достаточно ресурсоемко. Обычно в словарь попадают слова, 
имеющие наибольшую частоту использования в данном корпусе, 
либо просто все слова корпуса.

Те слова из корпуса, которые не попали в словарь заменяются специальным словом $UNK$, 
оно также добавляется в словарь.

Затем каждому слову $w$ из словаря сопоставляется целочисленный индекс $i_w$ 
от $0$ до $N-1$, где $N$~--- размер словаря.
Каждое слово в словаре задается вектором размерности $N$, 
в котором на позиции $i_w$ находится единица, а на остальных позиция~--- нули
Оперированием данным вектором напрямую также ресурсоемко, поэтому вводится вещественная матрица $W$, 
размерности $N \times d$, где $N >> d$. Тогда вектор для слова $w$ вычисляется как
$$a_w = E_{i_w} \cdot W$$
где $E$~--- единичная матрица $N \times N$.
Таким образом мы построили векторное представление для слов из словаря, которое задается матрицей $W$.

Матрица $W$ является обучаемым параметром, и инициализируется либо произвольными числами, 
либо же используются предобученные на больших корпусах данных векторные представления, 
такие как $word2vec$ или $GLoVe$ [ссылки].

Теперь необходимо для предложения построить дерево синтаксического разбора.
В рамках данной работы мы будем делать это, используя готовое решение $Stanford$ [ссылка].

При тестировании модели, производится аналогичная предобработка, 
с тем лишь отличием, что используется словарь, построенный по тренировочным данным.

Заметим, что весь процесс предобработки предложений автоматизирован, 
и не требует вмешательства человека.

\subsection{Описание идеи \textquote{локальный контекст слов}}
Минусом подхода РТНС является то обстоятельство, что для поддеревьев с небольшим количеством слов (до 4-5 слов), методу сложно построить векторное представление достаточно точно, 
ввиду отсутствия контекста использования слов. Так, в данном примере:

\todo{Пример}

Данная проблема наталкивает на мысль учитывать слова, использующиеся вместе с каждым словом в предложении, перед передачей в РНТС.

Формально, предложение задается матрицей в  $\mathbb{R}^{l \times{} d}$, в которой $i$-я строка равна векторному представлению слова на позиции $i$,\\
где $l$~--- количество слов в предложении,\\
$d$~--- размерность векторного представления слов. 

Наша задача: получить новую матрицу в $\mathbb{R}^{l \times {} t}$, где $i$-я строка задает векторное представление \textbf{\textit{контекста}} данного слова в предложении. 
Данная операция задается оператором $F:\mathbb{R}^{l \times d} \to \mathbb{R}^{l \times t}$.

В рамках данной работы мы рассмотрим такие операторы $F$, которые в качестве \textit{контекста} 
для слова $i$ используют смежные слова.\\
Формально: слова, на таких позициях $j$, что $i \le j < i + k$, где $k \in \mathbb{N}$~--- фиксированное значение и является параметром алгоритма.\\Оператор такого вида определяется оператором
$C:\mathbb{R}^{k \times d} \to \mathbb{R}^t$, который определяет способ 
получить векторное представление из смежных слов. \\
Тогда $$F(\pmb{X})_i = C(\pmb{X}'_{i..i+k-1})$$
где $\textbf{X}'$~--- это $\textbf{X}$ c добавленными в конец $k-1$ строками из нулей, \\
$\pmb{X}'_{i..i+k-1}$~--- матрица, образованная строками $\pmb{X}'$ c $i$ по $i+k-1$.\par

\vspace{5mm}

\noindent В данной работе будут использоваться следующие операторы $C$:
\begin{itemize}
    \item{полносвязный слой}
        $$C_{FC}(\pmb{X})=\sigma(\pmb{X}^T \cdot \pmb{w} + \pmb{b})$$
        где $\pmb{w} \in R^k, \pmb{b} \in R^d$, в данном случае $t=d$
    \item{сверточная нейронная сеть}
        $$c_j(\pmb{X})=\sigma(\pmb{X} \odot \pmb{m}_j + b_j)$$
        $$C_{CNN}(\pmb{X})=[c_1(\pmb{X}); c_2(\pmb{X}); \cdots c_t(\pmb{X})]$$
        где $\pmb{m}_j \in \mathbb{R}^{k \times d}, b_j \in \mathbb{R}$,\\
        $\odot$~--- поэлементное произведение и суммирование полученных произведений, 
        так называемая операция \textquote{свертки}
    \item{долгая краткосрочная память}
    $$\pmb{h}_i=LSTM_h(\pmb{c}_{i-1}, \pmb{h}_{i-1}, \pmb{X}_i) \text{ для } i \text{ от } 1 \text { до } k$$  
    $$\pmb{c}_i=LSTM_c(\pmb{c}_{i-1}, \pmb{h}_{i-1}, \pmb{X}_i) \text{ для } i \text{ от } 1 \text { до } k$$ 
    $$\pmb{c}_0 = \emptyset, \pmb{h}_0 = \emptyset$$
    $$C_{LSTM}(\pmb{X}) = \pmb{h}_k$$
    где $LSTM$~--- ячейка долгой краткосрочной памяти, описанной в разделе[]\\
    $LSTM_h, LSTM_c$~--- вычисление векторов $\pmb{c}$ и $\pmb{h}$ из $LSTM$ ячейки соответственно,\\
    $\pmb{h}_i \in \mathbb{R}^t, \pmb{c}_i \in \mathbb{R}^t$
\end{itemize}

\noindent Эта идея была названа \textquote{локальный контекст слов}.

\subsection{Описание идеи \textquote{значимые поддеревья}} 
Еще одной проблемой существующих решений, является то, 
что они учитывают все слова предложения, 
хотя достаточно большое количество слов не несет смысловой нагрузки для решаемой задачи.
Это требует от модели более избирательного анализа слов, а также порождает проблему \textquote{затухания градиента}.

\todo{Пример}

Данную идею можно обобщить на дерево разбора.

Перед нами стоит задача: вычислить для каждой вершины дерева разбора 
векторное представление фразы, соотвествующей этой вершине.
Мы будем делать это восходящим образом, вычисляя векторное представление для листьев,
затем для их предков, и так далее до корня дерева.

Обозначим векторное представление вершины $v$ за $\pmb{f}_v \in \mathbb{R}^s$.
Для вычисления $\pmb{f}_v$ в поддереве вершины $v$ будем выбирать такие поддеревья, 
что соответствующие им фразы являются наиболее значимыми для решаемой задачи.

Введем для каждой вершины $u$ весовой вектор $\pmb{w}_u \in \mathbb{R}^p$  так, что
чем больше квадрат нормы  этого вектора, тем более значима фраза, соотвествующая $u$.

Теперь чтобы посчитать векторное представление вершины $v$, выберем $K(v)$ вершин
в ее поддереве с наибольшими значениями $\lVert \pmb{w}_u \rVert^2$, пусть это $\{ u'_1, u'_2, \dots u'_{K(v)} \}$.
Затем с помощью некоторого оператора $G_v:\mathbb{R}^{K(v) \times s} \to \mathbb{R}^s$, 
передав в него выбранные значения $\pmb{f}_{u'_i}$, посчитаем векторное представление $\pmb{f}_v$.
Теперь остается пересчитать $\pmb{w}_v$. Сделаем это аналогичным образом, используя некоторый оператор 
$W_v :\mathbb{R}^{K(v) \times (s + p)} \to \mathbb{R}^p$, $\pmb{f}_{u'_i}$ и $\pmb{w}_{u'_i}$.

Формально:
$$TopK_v \{ \lVert \pmb{w}_{u_1} \rVert^2, \lVert \pmb{w}_{u_2} \rVert^2, \dots, \lVert \pmb{w}_{u_{2n-1}} \rVert^2\} = \{u'_1, u'_2, \dots, u'_{K(v)}\}$$
$$\pmb{f}_v = G_v(\pmb{f}_{u'_1}, \pmb{f}_{u'_2}, \dots, \pmb{f}_{u'_{K(v)}})$$
$$\pmb{w}_v = W_v(\pmb{f}_{u'_1} \circ \pmb{w}_{u'_1},\pmb{f}_{u'_2} \circ \pmb{w}_{u'_2}, \dots, \pmb{f}_{u'_{K(v)}} \circ \pmb{w}_{u'_{K(v)}})$$

\noindent $n$~--- количество слов в фразе, соответствующей вершине $v$\\
$u_1, u_2, \dots u_{2n-1}$~--- вершины поддерева $v$ в порядке эйлерова обхода\\
$TopK_v$~--- функция, которая выбирает $K(v)$ наибольших значений и возвращает их порядковые номера\\
$K(v)$~--- функция, которая определяет количество значимых поддеревьев для вершины $v$\\
$\circ$~--- операция конкатенации двух векторов в один

Мы можем видеть, что данный подход задается операторами $G_v$, $W_v$ и функцией $K$.

\todo{Что используется в качестве K}

\noindent Эта идея была названа \textquote{значимые поддеревья}.

\subsection{Полученная архитектура}

\subsection{Обучение}

\subsection{Архитектурные решения}
 