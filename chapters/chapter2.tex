%-*-coding: utf-8-*-

\chapter{Описание предложенного решения}

В данной главе будут рассмотрены предложенные идеи архитектур нейронных сетей на основе дерева разбора предложения.

В разделе 2.1 будут кратко описаны предложенные идеи и мотивация их использования.

В разделе 2.2 будет дано формальное описание модели.

\section{Предложенные идеи}

\subsection{Контекст слов в предложении}

Минусом подхода РТНС является то обстоятельство, что для поддеревьев с небольшим количеством слов (до 4-5 слов), методу сложно построить векторное представление достаточно точно, ввиду отсутствия контекста использования слов. 
Так, в данном примере:

\todo{Пример}

Данная проблема наталкивает на мысль учитывать слова, 
использующиеся вместе с каждым словом в предложении, перед передачей в РНТС.

\subsection{Значимые слова в предложении}
Еще одной проблемой существующих решений, является то,  что они учитывают все слова предложения, 
хотя достаточно большое количество слов не несет смысловой нагрузки для решаемой задачи, а также для построения взаимосвязей
между словами.
Это требует от модели более избирательного анализа слов, а также порождает проблему \textquote{затухания градиента}, из-за 
длинной последовательности вычислений.

Рассмотрим следующий примеры для задачи классификации эмоционального тона предложения на датасете, состоящем из
обзоров кинофильмов.

{\fontfamily{cmss}\selectfont A \textbf{screenplay more ingeniosly} constructed \textbf{than Memento}.}

{\fontfamily{cmss}\selectfont Suffers from the \textbf{lack} of a \textbf{compelling} or \textbf{comprehensible narrative}.}

{\fontfamily{cmss}\selectfont Still, this \textbf{flick is fun} and \textbf{host to} some truly \textbf{excellent sequences}.}

\noindent Жирным выделены слова, которе несут основную смысловую нагрузку предложения.\\
Можно видеть, что достаточно большой процент от общего количества слов в предложении составляют 
\textquote{незначимые} для задачи слова.

Данная идея как раз и состоит в выделении наиболее значимых слов и фраз в предложении.
Эту идею можно обобщить на дерево разбора.

\section{Формальное описание алгоритма}

\subsection{Предобработка данных}
Для того чтобы алгоритм смог проанализировать предложение, 
его необходимо привести к определенному формату. Мы рассмотрим обработку
тренировочных данных, обработка тестовых данных производится аналогично.

В первую очередь, предложение необходимо разбить на слова. 
Это можно сделать используя регулярные выражения. 

Затем по данному корпусу предложений необходимо построить словарь слов.
Словарь~--- это некоторое подмножество слов предложений корпуса.
Это делается для того, чтобы ограничить объем вычислений, так как использование всех
слов языка достаточно ресурсоемко. Обычно в словарь попадают слова, 
имеющие наибольшую частоту использования в данном корпусе, 
либо просто все слова корпуса.

Те слова из корпуса, которые не попали в словарь заменяются специальным словом $UNK$, 
оно также добавляется в словарь.

Затем каждому слову $w$ из словаря сопоставляется целочисленный индекс $i_w$ 
от $0$ до $N-1$, где $N$~--- размер словаря.
Каждое слово в словаре задается вектором размерности $N$, 
в котором на позиции $i_w$ находится единица, а на остальных позиция~--- нули
Оперированием данным вектором напрямую также ресурсоемко, поэтому вводится вещественная матрица $W$, 
размерности $N \times d$, где $N >> d$. Тогда вектор для слова $w$ вычисляется как
$$a_w = E_{i_w} \cdot W$$
где $E$~--- единичная матрица $N \times N$.
Таким образом мы построили векторное представление для слов из словаря, которое задается матрицей $W$.

Матрица $W$ является обучаемым параметром, и инициализируется либо произвольными числами, 
либо же используются предобученные на больших корпусах данных векторные представления, 
такие как $word2vec$ или $GLoVe$ [ссылки].

Теперь необходимо для предложения построить дерево синтаксического разбора.
В рамках данной работы мы будем делать это, используя готовое решение $Stanford$ [ссылка].

При тестировании модели, производится аналогичная предобработка, 
с тем лишь отличием, что используется словарь, построенный по тренировочным данным.

Заметим, что весь процесс предобработки предложений автоматизирован, 
и не требует вмешательства человека.

\subsection{Описание идеи \textquote{локальный контекст слов}}

Формально, предложение задается матрицей в  $\mathbb{R}^{l \times{} d}$, в которой $i$-я строка равна векторному представлению слова на позиции $i$,\\
где $l$~--- количество слов в предложении,\\
$d$~--- размерность векторного представления слов. 

Наша задача: получить новую матрицу в $\mathbb{R}^{l \times {} t}$, где $i$-я строка задает векторное представление \textit{контекста} данного слова в предложении. 
Данная операция задается оператором $F:\mathbb{R}^{l \times d} \to \mathbb{R}^{l \times t}$.

В рамках данной работы мы рассмотрим такие операторы $F$, которые в качестве контекста
для слова $i$ используют смежные слова.\\
Формально: слова, на таких позициях $j$, что $i \le j < i + k$, где $k \in \mathbb{N}$~--- фиксированное значение и является параметром алгоритма.\\Оператор такого вида определяется оператором
$C:\mathbb{R}^{k \times d} \to \mathbb{R}^t$, который определяет способ 
получить векторное представление из смежных слов. \\
Тогда $$F(\pmb{X})_i = C(\pmb{X}'_{i..i+k-1})$$
где $\pmb{X}'$~--- это $\pmb{X}$ c добавленными в конец $k-1$ строками из нулей, \\
$\pmb{X}'_{i..i+k-1}$~--- матрица, образованная строками $\pmb{X}'$ c $i$ по $i+k-1$.\par

\vspace{5mm}

\noindent В данной работе будут использоваться следующие операторы $C$:
\begin{itemize}
    \item{полносвязный слой}
        $$C_{FC}(\pmb{X})=\sigma(\pmb{X}^T \cdot \pmb{w} + \pmb{b})$$
        где $\pmb{w} \in R^k, \pmb{b} \in R^d$, в данном случае $t=d$
    \item{сверточная нейронная сеть}
        $$c_j(\pmb{X})=\sigma(\pmb{X} \odot \pmb{m}_j + b_j)$$
        $$C_{CNN}(\pmb{X})=[c_1(\pmb{X}); c_2(\pmb{X}); \cdots c_t(\pmb{X})]$$
        где $\pmb{m}_j \in \mathbb{R}^{k \times d}, b_j \in \mathbb{R}$,\\
        $\odot$~--- поэлементное произведение и суммирование полученных произведений, 
        так называемая операция \textquote{свертки}
    \item{долгая краткосрочная память}
    $$\pmb{h}_i=LSTM_h(\pmb{c}_{i-1}, \pmb{h}_{i-1}, \pmb{X}_i) \text{ для } i \text{ от } 1 \text { до } k$$  
    $$\pmb{c}_i=LSTM_c(\pmb{c}_{i-1}, \pmb{h}_{i-1}, \pmb{X}_i) \text{ для } i \text{ от } 1 \text { до } k$$ 
    $$\pmb{c}_0 = \emptyset, \pmb{h}_0 = \emptyset$$
    $$C_{LSTM}(\pmb{X}) = \pmb{h}_k$$
    где $LSTM$~--- ячейка долгой краткосрочной памяти, описанной в разделе[]\\
    $LSTM_h, LSTM_c$~--- вычисление векторов $\pmb{c}$ и $\pmb{h}$ из $LSTM$ ячейки соответственно,\\
    $\pmb{h}_i \in \mathbb{R}^t, \pmb{c}_i \in \mathbb{R}^t$
\end{itemize}

\noindent Эта идея была названа \textquote{локальный контекст слов}.

\subsection{Описание идеи \textquote{значимые поддеревья}} 

Перед нами стоит задача: вычислить для каждой вершины дерева разбора 
векторное представление фразы, соотвествующей этой вершине.
Мы будем делать это восходящим образом, вычисляя векторное представление для листьев,
затем для их предков, и так далее до корня дерева.

Обозначим векторное представление вершины $v$ за $\pmb{f}_v \in \mathbb{R}^s$.
Для вычисления $\pmb{f}_v$ в поддереве вершины $v$ будем выбирать такие поддеревья, 
что соответствующие им фразы являются наиболее значимыми для решаемой задачи.

Введем для каждой вершины $u$ весовой вектор $\pmb{w}_u \in \mathbb{R}^p$  так, что
чем больше квадрат нормы  этого вектора, тем более значима фраза, соотвествующая $u$.

Теперь чтобы посчитать векторное представление вершины $v$, выберем $K(v)$ вершин
в ее поддереве с наибольшими значениями $\lVert \pmb{w}_u \rVert^2$, пусть это $\{ u'_1, u'_2, \dots u'_{K(v)} \}$.
Затем с помощью некоторого оператора $G_v:\mathbb{R}^{K(v) \times s} \to \mathbb{R}^s$, 
передав в него выбранные значения $\pmb{f}_{u'_i}$, посчитаем векторное представление $\pmb{f}_v$.
Теперь остается пересчитать $\pmb{w}_v$. Сделаем это аналогичным образом, используя некоторый оператор 
$W_v :\mathbb{R}^{K(v) \times (s + p)} \to \mathbb{R}^p$, $\pmb{f}_{u'_i}$ и $\pmb{w}_{u'_i}$.

Формально:
$$TopK_v \{ \lVert \pmb{w}_{u_1} \rVert^2, \lVert \pmb{w}_{u_2} \rVert^2, \dots, \lVert \pmb{w}_{u_{2n-1}} \rVert^2\} = \{u'_1, u'_2, \dots, u'_{K(v)}\}$$
$$\pmb{f}_v = G_v(\pmb{f}_{u'_1}, \pmb{f}_{u'_2}, \dots, \pmb{f}_{u'_{K(v)}})$$
$$\pmb{w}_v = W_v(\pmb{f}_{u'_1} \circ \pmb{w}_{u'_1},\pmb{f}_{u'_2} \circ \pmb{w}_{u'_2}, \dots, \pmb{f}_{u'_{K(v)}} \circ \pmb{w}_{u'_{K(v)}})$$

\noindent $n$~--- количество слов в фразе, соответствующей вершине $v$\\
$u_1, u_2, \dots u_{2n-1}$~--- вершины поддерева $v$ в порядке эйлерова обхода\\
$TopK_v$~--- функция, которая выбирает $K(v)$ наибольших значений и возвращает их порядковые номера\\
$K(v)$~--- функция, которая определяет количество значимых поддеревьев для вершины $v$\\
$\circ$~--- операция конкатенации двух векторов в один

Мы можем видеть, что данный подход задается семействами операторов $G_v$ и $W_v$, и функцией $K$.

\todo{Что используется в качестве K}

\noindent Эта идея была названа \textquote{значимые поддеревья}.

\subsection{Архитектура и обучение модели}

Архитектуру полученной модели можно условно разбить на три части
\begin{enumerate}
    \item{подсчет векторного представления локального контекста}
    \item{подсчет векторного представления поддеревьев}
    \item{способ решение поставленной задачи по векторному представлению корня}
\end{enumerate}

Первый пункт был формально описан в разделе 2.2.2[].

Раздел 2.2.3 описывает предложенный механизм для подсчета векторного представления поддеревьев.
Помимо предложенного решения, в рамках данной работы используется простая рекуррентная модель пересчета поддеревьев [статья], которая задается как:
$$\pmb{f}_v = \sigma(\pmb{f}_l \cdot \pmb{W}_1 + \pmb{f}_r \cdot \pmb{W}_2 + \pmb{b})$$
где $\pmb{f}_v$~--- векторное представление вершины $v$, а $\pmb{f}_l$ и $\pmb{f}_r$~--- непосредственных
потомков веришы $v$,\\
$\pmb{W}_1, \pmb{W}_2 \in \mathbb{R}^{s \times s}$,
$\pmb{b} \in \mathbb{R}^s$~--- параметры рекуррентной модели

А также, древовидная LSTM модель [статья], которая задается как:
$$\tilde{i}_v=\sigma \left( U_1^{(i)} \cdot h_{v,1} + U_2^{(i)} \cdot h_{v,2} + b^{(i)} \right)$$
$$\tilde{f}_{vk}=\sigma \left( U_1^{(f)} \cdot h_{v,1} + U_2^{(f)} \cdot h_{v,2} + b^{(f)} \right),\text{ }k=1,2$$
$$\tilde{o}_{v}=\sigma \left( U_1^{(o)} \cdot h_{v,1} + U_2^{(o)} \cdot h_{v,2} + b^{(o)} \right)$$
$$\tilde{u}_{v}=\tanh \left( U_1^{(u)} \cdot h_{v,1} + U_2^{(u)} \cdot h_{v,2} + b^{(u)} \right)$$
$$\tilde{c}_v=\tilde{i}_v \odot \tilde{u}_v + \tilde{f}_{v,1} \cdot \tilde{c}_{v, 1} + \tilde{f}_{v,2} \cdot \tilde{c}_{v, 2}$$
$$\tilde{h}_v=\tilde{o}_v \cdot \tanh(\tilde{c}_v)$$

\subsection{Архитектурные решения}
 