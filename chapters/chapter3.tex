%-*-coding: utf-8-*-

\chapter{Эксперименты и результаты}

\section{Решаемые задачи}
В данном разделе будут описаны задачи, на которых будет проверяться качество предложенной модели.
\vspace{5mm}

\noindent \textbf{Задача классификации эмоционального тона предложения}\par
Задача классификации эмоционального тона предложения состоит в оценке эмоциональной характеристики предложения. В данной главе будут рассмотрены наборы данных, которые содержат обзоры фильмов. 
Предложенная модель должна будет предсказать, какому из пяти эмоциоанльных классов относится обзор:
\emph{очень негативный}, \emph{негативный}, \emph{нейтральный}, \emph{позитивный}, \emph{очень позитивный}.
\vspace{5mm}

\noindent \textbf{Задача классификации вопросов}\par
Задача классификации вопросов состоит в определении к какому типу принадлежит вопрос:
аббревиатура, сущность (животное, еда и т.д), описательный, личность, расположение, числовой.

\noindent \textbf{Задача классификации субъективных и объективных предложений}
Задача классификации субъективных и объективных предложений состоит в определении, является ли данное предложение объективным или субъективным. То есть является задачей бинарной классификации.

\section{Описание наборов данных и методики тестирования}
Предложенное решение было протестировано на нескольких задачах. 
Наборы данных содержат синтаксические деревья либо предложения. 
Для предложений были построены синтаскические деревья с помощью[ссылка].
\vspace{5mm}

\noindent \begin{minipage}{\linewidth}
\captionof{table}{\textbf{Наборы данных}} \label{tab:title} 
\begin{tabular}{|c|c|c|c|c|}
\hline
\multirow{2}{*}
  {Название}      & Кол-во         & Средняя длина          & Кол-во          & Кол-во  \\
                  & классов        & предложения            & трен. примеров  & тест. примеров \\ \hline
\textbf{SST-1}    & 5              & 18                     &  11855          &  2210    \\ \hline
\textbf{SST-2}    & 2              & 19                     &  9613           &  1821    \\ \hline
\textbf{Subj}     & 2              & 23                     &  10000          &  CV     \\ \hline
\textbf{TREC}     & 6              & 10                     &  5952           &  500    \\ \hline
\end{tabular}
\vspace{5mm}

\begin{itemize}
\item{\textbf{SST-1}, \textbf{SST-2}}~--- наборы данных с обзорами фильмов, содержат синтаксические деревья разбора\\
\item{\textbf{Subj}}~--- набор данных с субъективными и объективными утверждениями, содержит предложения\\
\item{\textbf{TREC}}~--- набор данных с шестью типами вопросов, содержит предложения
\end{itemize}
\end{minipage}
\vspace{5mm}

Для обучения был использован язык Python 3.4 и фреймворк символьных вычислений Tensorflow[ссылка].
Обучение проводилось с помощью оптимизаторов Adam[ссылка] и Adagrad[ссылка].

В процессе обучения архитектур была использована $L_2$ регуляризация[ссылка].
Это подход, когда к минимизируемой ошибке добавляются квадраты всех параметров сети, умноженных на некоторый коэффициент.
Он предотвращает переобучение параметров, так как параметры не могут принимать большие значения из-за увеличения ошибки.
$L_2$ регуляризация задается коэффициентом регуляризации $\lambda$.

Кроме того, использовался dropout (дропаут)[ссылка]. 
Эта техника заключается в том, что во время обучения некоторые нейроны с некоторой вероятностью не участвуют в предсказании.
Этот подход предотвращает cоадоптацию нейронов. Он задается вероятностью участия нейрона в предсказании $p$.

Обучение происходило минибатчами. Один минибатч включает в себя несколько тренировочных примеров, ошибка и градиент оптимизатора
берется как среднее ошибок и среднее градиентов по примерам в минибатче. Это сделано для более плавного роста ROC прямой и, следовательно, более быстрой сходимости. Размер минибатча составлял 25.

В датасетах \textbf{SST-1} и \textbf{SST-2} проаннатированы все поддеревья, 
поэтому для увеличения эффективности обучения ошибка учитывается не только от корня дерева, но и от всех поддеревьев.

\subsection{Тестирование архитектуры с локальными контекстами}
В данном разделе мы рассмотрим эксперименты над архитектурами, с вычислением локальных контекстов.

Сначала рассмотрим использование простой рекурсивной модели в качестве механизма пересчета поддеревьев.

\vspace{5mm}
\noindent \begin{minipage}{\linewidth}
\captionof{table}{\textbf{Вычисление локальных контекстов на основе CNN}} \label{tab:title} 
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{Набор}   &                \multicolumn{6}{c|}{Размер k-граммы} \\ \cline{2-7} 
     данных              & \textbf{-}   &   \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{2-5} \\ \hline
\textbf{SST-1}           & 45.5\%       & 46.7\%       & 46.3\%     & 46.1\%     &  46.1\%    &  47.2\%  \\ \hline
\textbf{SST-2}           &              &              &            &            &            & \\ \hline
\textbf{Subj}            & 88.8\%       & 90.9\%       & 90.8\%     & 90.9\%     &  90.5\%    & 91.1\% \\ \hline
\textbf{TREC}            & 89.2\%       & 89.9\%       & 92.4\%     & 91.1\%     &  90.9\%    & 90.3\% \\ \hline
\end{tabular}
\end{minipage}
\vspace{5mm}

\noindent \begin{minipage}{\linewidth}
\captionof{table}{\textbf{Вычисление локальных контекстов на основе LSTM}} \label{tab:title} 
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{Набор}   &                \multicolumn{6}{c|}{Размер k-граммы} \\ \cline{2-7} 
     данных              & \textbf{-}   &   \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{2-5} \\ \hline
\textbf{SST-1}           &  45.5\%      & 46.4\%       & 46.4\%     & 46.3\%     &  46.2\%    & 46.2\%           \\  \hline
\textbf{SST-2}           &              &              &            &            &            & \\ \hline
\textbf{Subj}            &  88.8\%      & 89.3\%       & 89.3\%     & 89.6\%     &  89.8\%    & 89.7\% \\ \hline
\textbf{TREC}            &  89.2\%      & 84.9\%       & 87.1\%     &            &            & 88.6\%  \\ \hline
\end{tabular}
\vspace{5mm}
\end{minipage}
\vspace{5mm}

Первый столбец описывает модель без использования локальных контекстов, то есть в качестве векторного представления локального контекста слова выступает векторное представление слова.

Последний столбец описывает модель, которая считает векторное представление $k$-грамм для $k=2,3,4,5$ и конкатенирует их.

Подход превосходит базовую модель РНТС[статья] на датасете \textbf{SST-1},
которая использует простой рекурсивный подсчет поддеревьев.

Мы можем видеть, что вычисление на основе CNN показывает себя хорошо при разных значениях $k$ в зависимости от задачи.
%Модели $2-4$ лучше работают на более длинных предложениях.
\todo{Зависимость от длины предложений}

Вычисление длинных $k$-грамм не приводит к хорошему результату, так как в контекст начинают попадать далекие слова, 
которые могут иметь достаточно произвольный характер.

Мы видим, что использование LSTM в качестве механизма захвата контекста значительно проигрывает CNN.

Вычисление на основе CNN обучается достаточно быстро, но требует кропотливой подборки гиперпараметров, 
таких как количество карт признаков, а также $\lambda$ и $p$.

Теперь рассмотрим в качестве механизма пересчета поддеревьев древовидный LSTM, 
чтобы показать, что захват локальных контекстов работает не только на одном методе, 
но может применяться и к более продвинутым моделям.

\vspace{5mm}
\noindent \begin{minipage}{\linewidth}
\captionof{table}{\textbf{Результаты TreeLSTM c контекстами CNN}} \label{tab:title} 
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{Набор}   &                \multicolumn{6}{c|}{Размер k-граммы} \\ \cline{2-7} 
     данных              & \textbf{-}   &   \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{2,3,4,5} \\ \hline
\textbf{SST-1}           &              &              &            &            &            & \\ \hline
\textbf{SST-2}           &              &              &            &            &            & \\ \hline
\textbf{Subj}            & 90.7\%       &  90.3\%      & 91.3\%     & 91.4\%     &   91.0\%   &  91.2\% \\ \hline
\textbf{TREC}            & 90.7\%       &  92.5\%      & 92.1\%     &            &            &  91.6\% \\ \hline
\end{tabular}
\end{minipage}
\vspace{5mm}

Здесь мы также можем видеть улучшение результатов при вычислении локальных контекстов.

\subsection{Тестирование архитектур со значимыми поддеревьями}

Напомним, что в подходе со значимыми поддеревьями, количество значимых поддеревьев определяется функцией $K$.

К сожалению, в ранмках Tensorflow сделать $K$ функцией не удалось, поэтому будут рассмотрены только постоянные значения $K$.

\vspace{5mm}
\noindent \begin{minipage}{\linewidth}
\captionof{table}{\textbf{Мда}} \label{tab:title} 
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{Набор}   &  \multicolumn{4}{c|}{Значения K} \\ \cline{2-5} 
     данных              &  \textbf{2}  & \textbf{4}   & \textbf{6} & \textbf{8} \\ \hline
\textbf{SST-1}           &              &              &            &            \\ \hline
\textbf{SST-2}           &              &              &            &            \\ \hline
\textbf{Subj}            &              &              &            &            \\ \hline
\textbf{TREC}            &              &              &            &            \\ \hline
\end{tabular}
\end{minipage}
\vspace{5mm}


Можно видеть, что данный подход имеет право на жизнь, но результаты оставляют желать лучшего.
На самом деле потенциал значимых поддеревьев раскроется в следующем разделе.

\subsection{Архитектуры с наилучшими результатами}

В данном разделе будут рассмотрены архитектуры, показавшие хорошие результаты на некоторых датасетах.

\vspace{5mm}
\noindent \begin{minipage}{\linewidth}
\captionof{table}{\textbf{Результаты TreeLSTM c контекстами CNN}} \label{tab:title} 
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{Набор}   &             \multicolumn{5}{c|}{Архитектуры} \\ \cline{2-6} 
     данных              &  \textbf{A}  & \textbf{B} & \textbf{C} & \textbf{D} & \textbf{E} \\ \hline
\textbf{SST-1}           &              &            &            &            &            \\ \hline
\textbf{SST-2}           &              &            &            &            &            \\ \hline
\textbf{Subj}            &              &            &            &            &            \\ \hline
\textbf{TREC}            &              &            &            &            &            \\ \hline
\end{tabular}
\end{minipage}
\vspace{5mm}

Архитектура \textbf{A}~--- локальный контекст считается \textbf{CNN-2,3,4}, в качестве механизма пересчета поддеревьев используется древовидная LSTM. После подсчета векторного представления $f_v$ поддерева вершины $v$ считается LSTM от слов поддерева, в которую помимо слов из поддерева подается $f_v$. То есть $f_v$ служит вспомогательным вектором для
LSTM. После чего пара из $(f_v, LSTM(v))$ подается в полносвязный классификатор.

Архитектура \textbf{B}