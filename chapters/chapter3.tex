%-*-coding: utf-8-*-

\chapter{Эксперименты и результаты}
 
\subsection{Описание наборов данных и методики тестирования}
 
Предложенное решение было протестировано на нескольких задачах. 
Наборы данных содержат синтаксические деревья либо предложения. 
Для предложений были построены синтаскические деревья с помощью[ссылка].
\vspace{5mm}

\noindent \begin{minipage}{\linewidth}
\captionof{table}{\textbf{Наборы данных}} \label{tab:title} 
\begin{tabular}{|c|c|c|c|c|}
\hline
\multirow{2}{*}
  {Название}      & Кол-во         & Средняя длина          & Кол-во          & Кол-во  \\
                  & классов        & предложения            & трен. примеров  & тест. примеров \\ \hline
\textbf{MR}       & 2              & 20                     &  10662          &  CV      \\ \hline
\textbf{SST-1}    & 5              & 18                     &  11855          &  2210    \\ \hline
\textbf{SST-2}    & 2              & 19                     &  9613           &  1821    \\ \hline
\textbf{Subj}     & 2              & 23                     &  10000          &  CV     \\ \hline
\textbf{TREC}     & 6              & 10                     &  5952           &  500    \\ \hline
\end{tabular}
\vspace{5mm}

\begin{itemize}
\item{\textbf{MR}} ~--- набор данных с обзорами фильмов, содержит предложения\\
\item{\textbf{SST-1}, \textbf{SST-2}}~--- наборы данных с обзорами фильмов, содержат синтаксические деревья разбора\\
\item{\textbf{Subj}}~--- набор данных с субъективными и объективными утверждениями, содержит предложения\\
\item{\textbf{TREC}}~--- набор данных с шестью типами вопросов, содержит предложения
\end{itemize}
\end{minipage}
\vspace{5mm}

Для обучения был использован язык Python 3.4 и фреймворк символьных вычислений Tensorflow[ссылка].
Обучение проводилось с помощью оптимизаторов Adam[ссылка] и Adagrad[ссылка].

В процессе обучения архитектур была использована $L_2$ регуляризация[ссылка].
Это подход, когда к минимизируемой ошибке добавляются квадраты всех параметров сети, умноженных на некоторый коэффициент.
Он предотвращает переобучение параметров, так как параметры не могут принимать большие значения из-за увеличения ошибки.
$L_2$ регуляризация задается коэффициентом регуляризации $\lambda$.

Кроме того, использовался dropout (дропаут)[ссылка]. 
Эта техника заключается в том, что во время обучения некоторые нейроны с некоторой вероятностью не участвуют в предсказании.
Этот подход предотвращает cоадоптацию нейронов. Он задается вероятностью участия нейрона в предсказании $p$.

Обучение происходило минибатчами. Один минибатч включает в себя несколько тренировочных примеров, ошибка и градиент оптимизатора
берется как среднее ошибок и среднее градиентов по примерам в минибатче. Это сделано для более плавного роста ROC прямой и, следовательно, более быстрой сходимости. Размер минибатча составлял 25.

В датасетах \textbf{SST-1} и \textbf{SST-2} проаннатированы все поддеревья, 
поэтому для увеличения эффективности обучения ошибка учитывается не только от корня дерева, но и от всех поддеревьев.

\subsection{Тестирование архитектуры с локальными контекстами}
В данном разделе мы рассмотрим эксперименты над архитектурами, с вычислением локальных контекстов.

Сначала рассмотрим использование простой рекурсивной модели в качестве механизма пересчета поддеревьев.

\vspace{5mm}
\noindent \begin{minipage}{\linewidth}
\captionof{table}{\textbf{Вычисление локальных контекстов на основе CNN}} \label{tab:title} 
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{Набор}   &                \multicolumn{5}{c|}{Размер k-граммы} \\ \cline{2-6} 
     данных              &   \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{2,3,4,5} \\ \hline
\textbf{MR}              &              &            &            &            &  \\ \hline
\textbf{SST-1}           & 46.7\%       & 46.3\%     &  46.1\%    &  46.1\%    &  47.2  \\ \hline
\textbf{SST-2}           & 2            & 19         &  9613      &  1821      & \\ \hline
\textbf{Subj}            & 2            & 23         &  10000     &  CV        & \\ \hline
\textbf{TREC}            & 6            & 10         &  5952      &  500       & \\ \hline
\end{tabular}
\end{minipage}
\vspace{5mm}

\noindent \begin{minipage}{\linewidth}
\captionof{table}{\textbf{Вычисление локальных контекстов на основе LSTM}} \label{tab:title} 
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{Набор}   &                \multicolumn{5}{c|}{Размер k-граммы} \\ \cline{2-6} 
     данных              &   \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{2,3,4,5} \\ \hline
\textbf{MR}              & 2            & 20         &  -     &  CV        &                      \\ \hline
\textbf{SST-1}           & 46.4\%       & 46.4\%     &  46.3\%    &  46.2\%    & \\ \hline
\textbf{SST-2}           & 2            & 19         &  9613      &  1821      & \\ \hline
\textbf{Subj}            & 2            & 23         &  10000     &  CV        & \\ \hline
\textbf{TREC}            & 6            & 10         &  5952      &  500       & \\ \hline
\end{tabular}\\
\vspace{5mm}
\end{minipage}
\vspace{5mm}

\todo{График тут мб?}

Последний столбец описывает модель, которая считает векторное представление $k$-грамм для $k=2,3,4,5$ и конкатенирует их.

Подход превосходит базовую модель РНТС на датасетах \textbf{SST-1} и \textbf{SST-2}, 
которая использует простой рекурсивный подсчет поддеревьев.

Мы можем видеть, что вычисление на основе CNN лучше работает на небольших размерах $k$-грамм.
Вычисление на основе LSTM немного лучше приспосабливается к большим окнам, хотя как и CNN ухудшает результат.
Это происходит потому что в контекст слов начинают попадать далекие слова, 
которые могут иметь достаточно произвольный характер.

Также можем видеть, что учет $2,3,4,5$-грамм значительно улучшает результат.

Несмотря на достаточно схожие результаты на основе CNN и LSTM, 
подход с CNN обучается значительно быстрее, нежели подход с LSTM, так как требует меньше вычислений.
Несмотря на это, LSTM-подход более устойчив к переобучению, и не требует кропотливой подборки $\lambda$ и $p$.

\todo{тут еще Tree-LSTM}

\subsection{Тестирование архитектуры со значимыми поддеревьями}

Напомним, что в подходе со значимыми поддеревьями, количество значимых поддеревьев определяется функцией $K$.

Сначала будут рассмотрены модели с постоянным значением $K$, не зависящим от размера поддерева.

\vspace{5mm}
\noindent \begin{minipage}{\linewidth}
\captionof{table}{\textbf{Постоянное количество значимых поддеревьев}} \label{tab:title} 
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{Набор}   &                \multicolumn{5}{c|}{Количество значимых поддеревьев} \\ \cline{2-6} 
     данных              &   \textbf{2} & \textbf{3} & \textbf{4} & \textbf{6} & \textbf{8} \\ \hline
\textbf{MR}              &              &            &            &            &  \\ \hline
\textbf{SST-1}           & 46.7\%       & 46.3\%     &  46.1\%    &  46.1\%    &  47.2  \\ \hline
\textbf{SST-2}           & 2            & 19         &  9613      &  1821      & \\ \hline
\textbf{Subj}            & 2            & 23         &  10000     &  CV        & \\ \hline
\textbf{TREC}            & 6            & 10         &  5952      &  500       & \\ \hline
\end{tabular}
\end{minipage}
\vspace{5mm}
